# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import LeakyReLU, Dense, Reshape, Conv2DTranspose, Conv2D, Flatten

# Define the generator model
def build_generator(latent_dim):
    model = tf.keras.Sequential()
    model.add(Dense(128 * 7 * 7, activation="relu", input_shape=(latent_dim,)))
    model.add(Reshape((7, 7, 128)))
    model.add(Conv2DTranspose(128, (5, 5), strides=(2, 2), padding="same", activation="relu"))
    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding="same", activation="relu"))
    model.add(Conv2D(1, (5, 5), padding="same", activation="sigmoid"))
    return model

# Define the discriminator model
def build_discriminator(input_shape):
    model = tf.keras.Sequential()
    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding="same", input_shape=input_shape))
    model.add(LeakyReLU(alpha=0.2))  # Use the imported LeakyReLU function
    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding="same"))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Flatten())
    model.add(Dense(1, activation="sigmoid"))
    return model

# Create the GAN model
def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = tf.keras.Sequential([generator, discriminator])
    return model

# Train the GAN model
def train_gan(gan, dataset, epochs, batch_size, latent_dim):
    # Compile the generator and discriminator models
    generator.compile(loss="binary_crossentropy", optimizer="adam")
    discriminator.compile(loss="binary_crossentropy", optimizer="adam")

    # Train the GAN for the specified number of epochs
    for epoch in range(epochs):
        for batch in dataset:
            # Train the discriminator
            discriminator.trainable = True
            real_images = batch
            fake_images = generator(tf.random.normal((batch_size, latent_dim)))
            discriminator_loss_real = discriminator(tf.reshape(real_images, (-1, 28, 28, 1)))  # Reshape real_images
            discriminator_loss_fake = discriminator(fake_images)
            discriminator_loss = tf.reduce_mean(discriminator_loss_real - discriminator_loss_fake)
            discriminator.train_on_batch(real_images, tf.ones_like(real_images))
            discriminator.train_on_batch(fake_images, tf.zeros_like(fake_images))

            # Train the generator
            discriminator.trainable = False
            generator_loss = gan(tf.random.normal((batch_size, latent_dim)))
            generator.train_on_batch(tf.random.normal((batch_size, latent_dim)), tf.ones_like(real_images))

        # Print epoch summary
        print(f"Epoch: {epoch + 1}/{epochs}")
